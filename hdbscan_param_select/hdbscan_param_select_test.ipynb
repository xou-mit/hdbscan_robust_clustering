{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XO's notes on 02/02/2022: This notebook is used to test the effect of parameter selection, \n",
    "# in particular, min_samples and min_cluster_size, on the\n",
    "# results of HDBSCAN on velocities/actions for\n",
    "# a typical sample of Gaia EDR3 astomety + DR2 radial velocities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.patches import Ellipse\n",
    "from matplotlib.colors import ListedColormap, Normalize, LogNorm\n",
    "from scipy.interpolate import interpn\n",
    "import seaborn as sns\n",
    "\n",
    "cmap = ListedColormap(sns.color_palette(\"colorblind\",256))\n",
    "\n",
    "label_size = 24\n",
    "matplotlib.rc('font', size=label_size) \n",
    "\n",
    "import sklearn\n",
    "# print(sklearn.__version__)\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import hdbscan\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"once\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "cm_vel_all = pd.read_hdf('../data/dr3_near_vel_plxzp_g2_only.h5')\n",
    "orb_param_all = pd.read_hdf('../data/dr3_orb_param_err_g2_only.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all -9999 values in APOGEE and RAVE6 cnn [Fe/H] and alpha to np.nan for easier calculation later\n",
    "ind_ap_999 = np.where(cm_vel_all.loc[:,'m_h_ap'] < -100)[0]\n",
    "ind_r6c_999 = np.where(cm_vel_all.loc[:,'m_h_r6c'] < -100)[0]\n",
    "ind_ap17_999 = np.where(cm_vel_all.loc[:,'m_h_ap17'] < -100)[0]\n",
    "cm_vel_all.loc[ind_ap_999,'m_h_ap'] = np.nan\n",
    "cm_vel_all.loc[ind_r6c_999,'m_h_r6c'] = np.nan\n",
    "cm_vel_all.loc[ind_ap17_999,'m_h_ap17'] = np.nan\n",
    "\n",
    "ind_ap_999 = np.where(cm_vel_all.loc[:,'m_h_err_ap'] < -100)[0]\n",
    "ind_ap17_999 = np.where(cm_vel_all.loc[:,'m_h_err_ap17'] < -100)[0]\n",
    "ind_r6c_999 = np.where(cm_vel_all.loc[:,'m_h_err_r6c'] < -100)[0]\n",
    "ind_l6s_999 = np.where(cm_vel_all.loc[:,'m_h_err_l6s'] < -100)[0]\n",
    "cm_vel_all.loc[ind_ap_999,'m_h_err_ap'] = np.nan\n",
    "cm_vel_all.loc[ind_ap17_999,'m_h_err_ap17'] = np.nan\n",
    "cm_vel_all.loc[ind_r6c_999,'m_h_err_r6c'] = np.nan\n",
    "cm_vel_all.loc[ind_l6s_999,'m_h_err_l6s'] = np.nan\n",
    "\n",
    "ind_ap_999 = np.where(cm_vel_all.loc[:,'alpha_m_ap'] < -100)[0]\n",
    "ind_ap17_999 = np.where(cm_vel_all.loc[:,'alpha_m_ap17'] < -100)[0]\n",
    "ind_r6c_999 = np.where(cm_vel_all.loc[:,'alpha_m_r6c'] < -100)[0]\n",
    "cm_vel_all.loc[ind_ap_999,'alpha_m_ap'] = np.nan\n",
    "cm_vel_all.loc[ind_ap17_999,'alpha_m_ap17'] = np.nan\n",
    "cm_vel_all.loc[ind_r6c_999,'alpha_m_r6c'] = np.nan\n",
    "\n",
    "ind_ap_999 = np.where(cm_vel_all.loc[:,'alpha_m_err_ap'] < -100)[0]\n",
    "ind_ap17_999 = np.where(cm_vel_all.loc[:,'alpha_m_err_ap17'] < -100)[0]\n",
    "ind_r6c_999 = np.where(cm_vel_all.loc[:,'alpha_m_err_r6c'] < -100)[0]\n",
    "ind_l6s_999 = np.where(cm_vel_all.loc[:,'alpha_m_err_l6s'] < -100)[0]\n",
    "cm_vel_all.loc[ind_ap_999,'alpha_m_err_ap'] = np.nan\n",
    "cm_vel_all.loc[ind_ap17_999,'alpha_m_err_ap17'] = np.nan\n",
    "cm_vel_all.loc[ind_r6c_999,'alpha_m_err_r6c'] = np.nan\n",
    "cm_vel_all.loc[ind_l6s_999,'alpha_m_err_l6s'] = np.nan\n",
    "\n",
    "# ASPCAP flag? Remove them later when making plots. We don't use them for clustering anyway\n",
    "# bad_bits = 2**23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the columns for the scaled action diamond\n",
    "orb_param_all['Jtot'] = np.sqrt(orb_param_all['Jphi']**2+orb_param_all['JR']**2+orb_param_all['Jz']**2)\n",
    "orb_param_all['diamond_x']=orb_param_all['Jphi']/orb_param_all['Jtot']\n",
    "orb_param_all['diamond_y']=(orb_param_all['Jz']-orb_param_all['JR'])/orb_param_all['Jtot']\n",
    "\n",
    "# Calculate the error for Jtot and diamond_x/y\n",
    "orb_param_all['e_Jtot'] = np.sqrt(orb_param_all['Jphi']**2*orb_param_all['e_Jphi']**2+orb_param_all['JR']**2*orb_param_all['e_JR']**2+orb_param_all['Jz']**2*orb_param_all['e_Jz']**2)/orb_param_all['Jtot']\n",
    "orb_param_all['e_diamond_x']=np.sqrt(orb_param_all['Jtot']**2*orb_param_all['e_Jphi']**2+orb_param_all['e_Jtot']**2*orb_param_all['Jphi']**2)/orb_param_all['Jtot']**2\n",
    "orb_param_all['e_diamond_y']=np.sqrt(orb_param_all['Jtot']**2*(orb_param_all['e_JR']**2+orb_param_all['e_Jz']**2)+orb_param_all['e_Jtot']**2*(orb_param_all['Jz']-orb_param_all['JR'])**2)/orb_param_all['Jtot']**2\n",
    "\n",
    "# Calculate L_perp for clustering\n",
    "orb_param_all['Lperp'] = np.sqrt(orb_param_all['Lx']**2+orb_param_all['Ly']**2)\n",
    "orb_param_all['e_Lperp'] = np.sqrt(orb_param_all['Lx']**2*orb_param_all['e_Lx']**2+orb_param_all['Ly']**2*orb_param_all['e_Ly']**2)/orb_param_all['Lperp']\n",
    "\n",
    "# This mean metallicity below is only used in the case when we actually want to cluster in or with a prior cut on metallicity\n",
    "# To avoid systematic differences between spectroscopic surveys; don't mix metallicities from different surveys\n",
    "# cm_vel_all['m_h_mean'] = np.nanmean(cm_vel_all[['m_h_ap17','m_h_l6s','m_h_r6c','m_h_gl3']].values, axis=1).T\n",
    "cm_vel_all['m_h_mean'], cm_vel_all['e_m_h_mean'] = cm_vel_all['m_h_ap17'], cm_vel_all['m_h_err_ap17'] \n",
    "\n",
    "orb_param_all['PCA_X'] = np.empty(len(orb_param_all))*np.nan\n",
    "orb_param_all['PCA_Y'] = np.empty(len(orb_param_all))*np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweak with the axes that go into PCA\n",
    "\n",
    "scaler = 'Robust' # 'Standard' or 'Robust' or None\n",
    "denoise = None # 'PCA' or 'AE' or None\n",
    "algorithm = 'HDBSCAN' # 'OPTICS' or 'DBSCAN' or 'HDBSCAN' or 'AGG_n' or 'AGG_l' or 'GMM'\n",
    "# Define what axes go into PCA\n",
    "action = True\n",
    "diamond = False\n",
    "metallicity = False\n",
    "velocity = False\n",
    "cylindrical = True\n",
    "position = False\n",
    "energy = False\n",
    "eccentricity = False\n",
    "Lz = False\n",
    "Lperp = False\n",
    "\n",
    "# Define what axes go into GMM with PCA results\n",
    "# By default, include whatever was not in PCA\n",
    "action_add = not action\n",
    "metallicity_add = not metallicity\n",
    "velocity_add = not velocity\n",
    "position_add = not position\n",
    "energy_add = not energy\n",
    "eccentricity_add = not eccentricity\n",
    "Lz_add = not Lz\n",
    "Lperp_add = not Lperp\n",
    "\n",
    "# Manual override additional axes\n",
    "# action_add = False\n",
    "position_add = False\n",
    "metallicity_add = False\n",
    "velocity_add = False\n",
    "energy_add = False\n",
    "eccentricity_add = False\n",
    "Lz_add = False\n",
    "Lperp_add = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27921\n"
     ]
    }
   ],
   "source": [
    "# My selection\n",
    "cutoff = 2500\n",
    "# Pick the stars with reasonable velocities(need to jutify this later)\n",
    "# and reasoanable metallicity \n",
    "# and meet all quality cuts except for the binary cut\n",
    "# Temporarily ignoring any action flags and not using any action for now\n",
    "kin_qual = ((abs(cm_vel_all['U_g2']) < 1000) & \n",
    "            (abs(cm_vel_all['V_g2']) < 1000) & (abs(cm_vel_all['W_g2']) < 1000) & \n",
    "            (cm_vel_all['qual_flag'] == 0) # & \n",
    "           )\n",
    "feh_qual = (((cm_vel_all['m_h_ap17'] > -10.0) | (cm_vel_all['m_h_r6c'] > -10.0) |\n",
    "             (cm_vel_all['m_h_gl3'] > -10.0) | (cm_vel_all['m_h_l6s'] > -10.0)) &\n",
    "            ((cm_vel_all['m_h_err_ap17'] > 0.0) | (cm_vel_all['m_h_err_r6c'] > 0.0) |\n",
    "             (cm_vel_all['m_h_err_gl3'] > 0.0) | (cm_vel_all['m_h_err_l6s'] > 0.0)))\n",
    "\n",
    "# act_qual = ((orb_param_all['flag_fail'] == 0) &\n",
    "#             (orb_param_all['flag_unbound'] == 0) & \n",
    "#             (orb_param_all['flag_circ'] == 0) &\n",
    "#             (orb_param_all['flag_act_conv'] == 0)\n",
    "#            )\n",
    "\n",
    "\n",
    "# selection = ((abs(orb_param_all['zmax']) > cutoff) & (cm_vel_all['vphi_g2'] > 100))\n",
    "# selection = ((abs(orb_param_all['zmax']) > cutoff) & (cm_vel_all['m_h_mean'] < -1.))\n",
    "selection = ((orb_param_all['zmax']-2*orb_param_all['e_zmax']) > cutoff)\n",
    "\n",
    "if metallicity == False & metallicity_add == False:\n",
    "    combined_cut = kin_qual & selection\n",
    "else:\n",
    "    print('Applying metallicity quality cut...')\n",
    "    combined_cut = kin_qual & selection & feh_qual\n",
    "\n",
    "ind_cut = np.where(combined_cut)[0]\n",
    "print(len(ind_cut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut the dataframe\n",
    "df_cut_vel = cm_vel_all.loc[ind_cut,:]\n",
    "df_cut_orb = orb_param_all.loc[ind_cut,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Axes not in PCA: ['Act_3d']\n",
      "Final clustering axes: ['Act_3d']\n",
      "Shape of X after adding additional axes: (27921, 3)\n",
      "Extra text is: 0202_g2only_err_act_2500\n"
     ]
    }
   ],
   "source": [
    "extratext = '0202_g2only_err'\n",
    "\n",
    "if denoise == 'PCA':\n",
    "    if action_add == True:\n",
    "        extratext += '_act'\n",
    "\n",
    "    if metallicity_add == True:\n",
    "        extratext += '_feh'\n",
    "\n",
    "    if velocity_add == True:\n",
    "        extratext += '_vel'\n",
    "\n",
    "    if position_add == True:\n",
    "        extratext += '_pos'\n",
    "\n",
    "    if energy_add == True:\n",
    "        extratext += '_etot'\n",
    "\n",
    "    if eccentricity_add == True:\n",
    "        extratext += '_ecc'\n",
    "\n",
    "    if Lz_add == True:\n",
    "        extratext += '_Lz'\n",
    "        \n",
    "    if Lperp_add == True:\n",
    "        extratext += '_Lperp'\n",
    "else:\n",
    "    if action == True or action_add == True:\n",
    "        extratext += '_act'\n",
    "        if diamond == True:\n",
    "            extratext += '_diamond'\n",
    "\n",
    "    if metallicity == True or metallicity_add == True:\n",
    "        extratext += '_feh'\n",
    "\n",
    "    if velocity == True or velocity_add == True:\n",
    "        extratext += '_vel'\n",
    "        if cylindrical == True:\n",
    "            extratext += '_cyl'\n",
    "\n",
    "    if position == True or position_add == True:\n",
    "        extratext += '_pos'\n",
    "\n",
    "    if energy == True or energy_add == True:\n",
    "        extratext += '_etot'\n",
    "\n",
    "    if eccentricity == True or eccentricity_add == True:\n",
    "        extratext += '_ecc'\n",
    "\n",
    "    if Lz == True or Lz_add == True:\n",
    "        extratext += '_Lz'\n",
    "        \n",
    "    if Lperp == True or Lperp_add == True:\n",
    "        extratext += '_Lperp'\n",
    "\n",
    "\n",
    "extratext += '_'+str(cutoff)\n",
    "\n",
    "\n",
    "# Make a list that records what axes are fed into PCA (or not)\n",
    "pca_list = []\n",
    "\n",
    "\n",
    "# Add in the dimensions as needed \n",
    "ydata_ini = []\n",
    "ydata_ini_err = []\n",
    "\n",
    "if action == True and diamond == True:\n",
    "    ydata_ini.append(df_cut_orb['diamond_x'])\n",
    "    ydata_ini.append(df_cut_orb['diamond_y'])\n",
    "    ydata_ini_err.append(df_cut_orb['e_diamond_x'])\n",
    "    ydata_ini_err.append(df_cut_orb['e_diamond_y'])\n",
    "    pca_list.append('Act_diamond')\n",
    "    \n",
    "if action == True and diamond == False:\n",
    "    ydata_ini.append(df_cut_orb['JR'])\n",
    "    ydata_ini.append(df_cut_orb['Jphi'])\n",
    "    ydata_ini.append(df_cut_orb['Jz'])\n",
    "    ydata_ini_err.append(df_cut_orb['e_JR'])\n",
    "    ydata_ini_err.append(df_cut_orb['e_Jphi'])\n",
    "    ydata_ini_err.append(df_cut_orb['e_Jz'])\n",
    "    pca_list.append('Act_3d')\n",
    "\n",
    "    \n",
    "if velocity == True and cylindrical == False:\n",
    "    ydata_ini.append(df_cut_vel[\"U_g2\"])\n",
    "    ydata_ini.append(df_cut_vel[\"V_g2\"])\n",
    "    ydata_ini.append(df_cut_vel[\"W_g2\"])\n",
    "    ydata_ini_err.append(df_cut_vel[\"Uerr_g2\"])\n",
    "    ydata_ini_err.append(df_cut_vel[\"Verr_g2\"])\n",
    "    ydata_ini_err.append(df_cut_vel[\"Werr_g2\"])\n",
    "    pca_list.append('Vel_cart')\n",
    "    \n",
    "if metallicity == True:\n",
    "    ydata_ini.append(df_cut_vel['m_h_mean'])\n",
    "    ydata_ini_err.append(df_cut_vel['e_m_h_mean'])\n",
    "    pca_list.append('[Fe/H]')\n",
    "    \n",
    "if velocity == True and cylindrical == True:\n",
    "    ydata_ini.append(df_cut_vel[\"vr_g2\"])\n",
    "    ydata_ini.append(df_cut_vel[\"vphi_g2\"])\n",
    "    ydata_ini.append(df_cut_vel[\"vz_g2\"])\n",
    "    ydata_ini_err.append(df_cut_vel[\"vrerr_g2\"])\n",
    "    ydata_ini_err.append(df_cut_vel[\"vphierr_g2\"])\n",
    "    ydata_ini_err.append(df_cut_vel[\"vzerr_g2\"])\n",
    "    pca_list.append('Vel_cyl')\n",
    "    \n",
    "if position == True:\n",
    "    ydata_ini.append(df_cut_vel[\"XGC\"])\n",
    "    ydata_ini.append(df_cut_vel[\"YGC\"])\n",
    "    ydata_ini.append(df_cut_vel[\"ZGC\"])\n",
    "    ydata_ini_err.append(df_cut_vel[\"XGCerr\"])\n",
    "    ydata_ini_err.append(df_cut_vel[\"YGCerr\"])\n",
    "    ydata_ini_err.append(df_cut_vel[\"ZGCerr\"])\n",
    "    pca_list.append('Pos_cart')\n",
    "\n",
    "if energy == True:\n",
    "    ydata_ini.append(df_cut_orb[\"Etot\"])\n",
    "    ydata_ini_err.append(df_cut_orb[\"e_Etot\"])\n",
    "    pca_list.append('E_tot')\n",
    "\n",
    "if eccentricity == True:\n",
    "    ydata_ini.append(df_cut_orb[\"ecc\"])\n",
    "    ydata_ini_err.append(df_cut_orb[\"e_ecc\"])\n",
    "    pca_list.append('ecc')\n",
    "\n",
    "if Lz == True:\n",
    "    ydata_ini.append(df_cut_orb[\"Lz\"])\n",
    "    ydata_ini_err.append(df_cut_orb[\"e_Lz\"])\n",
    "    pca_list.append('Lz')\n",
    "\n",
    "if Lperp == True:\n",
    "    ydata_ini.append(df_cut_orb[\"Lperp\"])\n",
    "    ydata_ini_err.append(df_cut_orb[\"e_Lperp\"])\n",
    "    pca_list.append('Lperp')\n",
    "    \n",
    "\n",
    "\n",
    "ydata = np.array(ydata_ini).T\n",
    "ydata_err = np.array(ydata_ini_err).T\n",
    "\n",
    "    \n",
    "\n",
    "if scaler == 'Standard':\n",
    "    X = StandardScaler().fit_transform(ydata)\n",
    "elif scaler == 'Robust':\n",
    "    X = RobustScaler().fit_transform(ydata)\n",
    "elif scaler == None:\n",
    "    X = ydata\n",
    "  \n",
    "\n",
    "\n",
    "print(\"Axes not in PCA:\",pca_list)\n",
    "# Put some place holder into the orbital param df\n",
    "df_cut_orb['PCA_X'] = np.empty(len(df_cut_orb))*np.nan\n",
    "df_cut_orb['PCA_Y'] = np.empty(len(df_cut_orb))*np.nan\n",
    "df_cut_orb['e_PCA_X'] = np.empty(len(df_cut_orb))*np.nan\n",
    "df_cut_orb['e_PCA_Y'] = np.empty(len(df_cut_orb))*np.nan\n",
    "axes_labels = pca_list.copy()\n",
    "\n",
    "\n",
    "\n",
    "# X_tp = X.T\n",
    "\n",
    "# # Combine the result with the non-PCA axes and store the correponding uncertainties into a separate \n",
    "# # Do vstack with .T transpose twice!\n",
    "# if action_add == True and diamond == True:\n",
    "#     X_tp = np.vstack((X_tp,df_cut_orb['diamond_x']))\n",
    "#     X_tp = np.vstack((X_tp,df_cut_orb['diamond_y']))\n",
    "#     X_err_tp = np.vstack((X_err_tp,df_cut_orb['e_diamond_x']))\n",
    "#     X_err_tp = np.vstack((X_err_tp,df_cut_orb['e_diamond_y']))\n",
    "#     axes_labels.append('Act_diamond')\n",
    "    \n",
    "# if action_add == True and diamond == False:\n",
    "#     X_tp = np.vstack((X_tp,df_cut_orb['JR']))\n",
    "#     X_tp = np.vstack((X_tp,df_cut_orb['Jphi']))\n",
    "#     X_tp = np.vstack((X_tp,df_cut_orb['Jz']))\n",
    "#     X_err_tp = np.vstack((X_err_tp,df_cut_orb['e_JR']))\n",
    "#     X_err_tp = np.vstack((X_err_tp,df_cut_orb['e_Jphi']))\n",
    "#     X_err_tp = np.vstack((X_err_tp,df_cut_orb['e_Jz']))\n",
    "#     axes_labels.append('Act_3d')\n",
    "\n",
    "# if metallicity_add == True:\n",
    "#     X_tp = np.vstack((X_tp,df_cut_vel['m_h_mean']))\n",
    "#     X_err_tp = np.vstack((X_err_tp,df_cut_vel['e_m_h_mean']))\n",
    "#     axes_labels.append('[Fe/H]')\n",
    "    \n",
    "# if velocity_add == True and cylindrical == False:\n",
    "#     X_tp = np.vstack((X_tp,df_cut_vel[\"U_g2\"]))\n",
    "#     X_tp = np.vstack((X_tp,df_cut_vel[\"V_g2\"]))\n",
    "#     X_tp = np.vstack((X_tp,df_cut_vel[\"W_g2\"]))\n",
    "#     X_err_tp = np.vstack((X_err_tp,df_cut_vel[\"Uerr_g2\"]))\n",
    "#     X_err_tp = np.vstack((X_err_tp,df_cut_vel[\"Verr_g2\"]))\n",
    "#     X_err_tp = np.vstack((X_err_tp,df_cut_vel[\"Werr_g2\"]))\n",
    "#     axes_labels.append('Vel_cart')\n",
    "    \n",
    "# if velocity_add == True and cylindrical == True:\n",
    "#     X_tp = np.vstack((X_tp,df_cut_vel[\"vr_g2\"]))\n",
    "#     X_tp = np.vstack((X_tp,df_cut_vel[\"vphi_g2\"]))\n",
    "#     X_tp = np.vstack((X_tp,df_cut_vel[\"vz_g2\"]))\n",
    "#     X_err_tp = np.vstack((X_err_tp,df_cut_vel[\"vrerr_g2\"]))\n",
    "#     X_err_tp = np.vstack((X_err_tp,df_cut_vel[\"vphierr_g2\"]))\n",
    "#     X_err_tp = np.vstack((X_err_tp,df_cut_vel[\"vzerr_g2\"]))\n",
    "#     axes_labels.append('Vel_cyl')\n",
    "    \n",
    "# if position_add == True:\n",
    "#     X_tp = np.vstack((X_tp,df_cut_vel[\"XGC\"]))\n",
    "#     X_tp = np.vstack((X_tp,df_cut_vel[\"YGC\"]))\n",
    "#     X_tp = np.vstack((X_tp,df_cut_vel[\"ZGC\"]))\n",
    "#     X_err_tp = np.vstack((X_err_tp,df_cut_vel[\"XGCerr\"]))\n",
    "#     X_err_tp = np.vstack((X_err_tp,df_cut_vel[\"YGCerr\"]))\n",
    "#     X_err_tp = np.vstack((X_err_tp,df_cut_vel[\"ZGCerr\"]))\n",
    "#     axes_labels.append('Pos_cart')\n",
    "\n",
    "# if energy_add == True:\n",
    "#     X_tp = np.vstack((X_tp,df_cut_orb[\"Etot\"]))\n",
    "#     X_err_tp = np.vstack((X_err_tp,df_cut_orb[\"e_Etot\"]))\n",
    "#     axes_labels.append('E_tot')\n",
    "\n",
    "# if eccentricity_add == True:\n",
    "#     X_tp = np.vstack((X_tp,df_cut_orb[\"ecc\"]))\n",
    "#     X_err_tp = np.vstack((X_err_tp,df_cut_orb[\"e_ecc\"]))\n",
    "#     axes_labels.append('ecc')\n",
    "\n",
    "# if Lz_add == True:\n",
    "#     X_tp = np.vstack((X_tp,df_cut_orb[\"Lz\"]))\n",
    "#     X_err_tp = np.vstack((X_err_tp,df_cut_orb[\"e_Lz\"]))\n",
    "#     axes_labels.append('Lz')\n",
    "\n",
    "# if Lperp_add == True:\n",
    "#     X_tp = np.vstack((X_tp,df_cut_orb[\"Lperp\"]))\n",
    "#     X_err_tp = np.vstack((X_err_tp,df_cut_orb[\"e_Lperp\"]))\n",
    "#     axes_labels.append('Lperp')\n",
    "    \n",
    "    \n",
    "# Put everything through Scaler again\n",
    "# if scaler == 'Standard':\n",
    "#     X = StandardScaler().fit_transform(X_tp.T)\n",
    "#     X_err = StandardScaler().fit(X_tp.T).transform(X_err_tp.T)\n",
    "# elif scaler == 'Robust':\n",
    "#     X = RobustScaler().fit_transform(X_tp.T)\n",
    "#     X_err = RobustScaler().fit(X_tp.T).transform(X_err_tp.T)\n",
    "# elif scaler == None:\n",
    "#     X = X_tp.T\n",
    "#     X_err = X_err_tp.T\n",
    "    \n",
    "print(\"Final clustering axes:\",axes_labels)\n",
    "print(\"Shape of X after adding additional axes:\",np.shape(X))\n",
    "print(\"Extra text is:\",extratext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting 3d action...\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "if denoise == 'PCA':\n",
    "    print('counting PCA...')\n",
    "    n_dim=2\n",
    "else:\n",
    "    n_dim=0\n",
    "    if metallicity == True:\n",
    "        print('counting Metallicity...')\n",
    "        n_dim += 1\n",
    "    if action == True and diamond == True:\n",
    "        print('counting action diamond...')\n",
    "        n_dim += 2\n",
    "    if action == True and diamond == False:\n",
    "        print('counting 3d action...')\n",
    "        n_dim += 3\n",
    "    if velocity == True and cylindrical == False:\n",
    "        print('counting cartesian velocity...')\n",
    "        n_dim += 3\n",
    "    if velocity == True and cylindrical == True:\n",
    "        print('counting cylindrical velocity...')\n",
    "        n_dim += 3\n",
    "    if position == True:\n",
    "        print('counting positions...')\n",
    "        n_dim += 3\n",
    "    if energy == True:\n",
    "        print('counting energy...')\n",
    "        n_dim += 1\n",
    "    if eccentricity == True:\n",
    "        print('counting eccentricity...')\n",
    "        n_dim += 1\n",
    "    if Lz == True:\n",
    "        print('counting Lz...')\n",
    "        n_dim += 1\n",
    "    if Lperp == True:\n",
    "        print('counting Lperp...')\n",
    "        n_dim += 1\n",
    "\n",
    "if metallicity_add == True:\n",
    "    print('counting Metallicity...')\n",
    "    n_dim += 1\n",
    "if action_add == True and diamond == True:\n",
    "    print('counting action diamond...')\n",
    "    n_dim += 2\n",
    "if action_add == True and diamond == False:\n",
    "    print('counting 3d action...')\n",
    "    n_dim += 3\n",
    "if velocity_add == True:\n",
    "    print('counting cartesian velocity...')\n",
    "    n_dim += 3\n",
    "if position_add == True:\n",
    "    print('counting positions...')\n",
    "    n_dim += 3\n",
    "if energy_add == True:\n",
    "    print('counting energy...')\n",
    "    n_dim += 1\n",
    "if eccentricity_add == True:\n",
    "    print('counting eccentricity...')\n",
    "    n_dim += 1\n",
    "if Lz_add == True:\n",
    "    print('counting Lz...')\n",
    "    n_dim += 1\n",
    "if Lperp_add == True:\n",
    "    print('counting Lperp...')\n",
    "    n_dim += 1\n",
    "    \n",
    "print(n_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering options\n",
    "extratext += '_hdbscan'\n",
    "cluster_selection_method = 'eom'\n",
    "extratext += '_'+str(cluster_selection_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_dir = './tree_plot_act_3d_2500/eom/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_arr = np.arange(2,20,2)\n",
    "min_cluster_size_arr = np.arange(20,200,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  4  6  8 10 12 14 16 18] [ 20  40  60  80 100 120 140 160 180]\n",
      "0202_g2only_err_act_2500_hdbscan_eom\n"
     ]
    }
   ],
   "source": [
    "print(min_samples_arr,min_cluster_size_arr)\n",
    "print(extratext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For min_samples= 2  and min_cluster_size= 20\n",
      "Estimated number of clusters: 6\n",
      "Estimated number of noise points: 1262\n",
      "For min_samples= 2  and min_cluster_size= 40\n",
      "Estimated number of clusters: 3\n",
      "Estimated number of noise points: 970\n",
      "For min_samples= 2  and min_cluster_size= 60\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 2075\n",
      "For min_samples= 2  and min_cluster_size= 80\n",
      "Estimated number of clusters: 8\n",
      "Estimated number of noise points: 11448\n",
      "For min_samples= 2  and min_cluster_size= 100\n",
      "Estimated number of clusters: 4\n",
      "Estimated number of noise points: 11791\n",
      "For min_samples= 2  and min_cluster_size= 120\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12016\n",
      "For min_samples= 2  and min_cluster_size= 140\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12016\n",
      "For min_samples= 2  and min_cluster_size= 160\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12016\n",
      "For min_samples= 2  and min_cluster_size= 180\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12016\n",
      "For min_samples= 4  and min_cluster_size= 20\n",
      "Estimated number of clusters: 5\n",
      "Estimated number of noise points: 1181\n",
      "For min_samples= 4  and min_cluster_size= 40\n",
      "Estimated number of clusters: 10\n",
      "Estimated number of noise points: 7093\n",
      "For min_samples= 4  and min_cluster_size= 60\n",
      "Estimated number of clusters: 7\n",
      "Estimated number of noise points: 16696\n",
      "For min_samples= 4  and min_cluster_size= 80\n",
      "Estimated number of clusters: 3\n",
      "Estimated number of noise points: 16976\n",
      "For min_samples= 4  and min_cluster_size= 100\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12583\n",
      "For min_samples= 4  and min_cluster_size= 120\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12583\n",
      "For min_samples= 4  and min_cluster_size= 140\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12583\n",
      "For min_samples= 4  and min_cluster_size= 160\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12583\n",
      "For min_samples= 4  and min_cluster_size= 180\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12583\n",
      "For min_samples= 6  and min_cluster_size= 20\n",
      "Estimated number of clusters: 11\n",
      "Estimated number of noise points: 3491\n",
      "For min_samples= 6  and min_cluster_size= 40\n",
      "Estimated number of clusters: 7\n",
      "Estimated number of noise points: 5856\n",
      "For min_samples= 6  and min_cluster_size= 60\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 3614\n",
      "For min_samples= 6  and min_cluster_size= 80\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 3614\n",
      "For min_samples= 6  and min_cluster_size= 100\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12704\n",
      "For min_samples= 6  and min_cluster_size= 120\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12704\n",
      "For min_samples= 6  and min_cluster_size= 140\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12704\n",
      "For min_samples= 6  and min_cluster_size= 160\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 26310\n",
      "For min_samples= 6  and min_cluster_size= 180\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 26310\n",
      "For min_samples= 8  and min_cluster_size= 20\n",
      "Estimated number of clusters: 3\n",
      "Estimated number of noise points: 1313\n",
      "For min_samples= 8  and min_cluster_size= 40\n",
      "Estimated number of clusters: 3\n",
      "Estimated number of noise points: 3721\n",
      "For min_samples= 8  and min_cluster_size= 60\n",
      "Estimated number of clusters: 3\n",
      "Estimated number of noise points: 12874\n",
      "For min_samples= 8  and min_cluster_size= 80\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12948\n",
      "For min_samples= 8  and min_cluster_size= 100\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12948\n",
      "For min_samples= 8  and min_cluster_size= 120\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12948\n",
      "For min_samples= 8  and min_cluster_size= 140\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 26220\n",
      "For min_samples= 8  and min_cluster_size= 160\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 26220\n",
      "For min_samples= 8  and min_cluster_size= 180\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 26220\n",
      "For min_samples= 10  and min_cluster_size= 20\n",
      "Estimated number of clusters: 9\n",
      "Estimated number of noise points: 5918\n",
      "For min_samples= 10  and min_cluster_size= 40\n",
      "Estimated number of clusters: 8\n",
      "Estimated number of noise points: 12316\n",
      "For min_samples= 10  and min_cluster_size= 60\n",
      "Estimated number of clusters: 3\n",
      "Estimated number of noise points: 12568\n",
      "For min_samples= 10  and min_cluster_size= 80\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12646\n",
      "For min_samples= 10  and min_cluster_size= 100\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12646\n",
      "For min_samples= 10  and min_cluster_size= 120\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12646\n",
      "For min_samples= 10  and min_cluster_size= 140\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12646\n",
      "For min_samples= 10  and min_cluster_size= 160\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12646\n",
      "For min_samples= 10  and min_cluster_size= 180\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12646\n",
      "For min_samples= 12  and min_cluster_size= 20\n",
      "Estimated number of clusters: 9\n",
      "Estimated number of noise points: 6382\n",
      "For min_samples= 12  and min_cluster_size= 40\n",
      "Estimated number of clusters: 5\n",
      "Estimated number of noise points: 12801\n",
      "For min_samples= 12  and min_cluster_size= 60\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12971\n",
      "For min_samples= 12  and min_cluster_size= 80\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12971\n",
      "For min_samples= 12  and min_cluster_size= 100\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12971\n",
      "For min_samples= 12  and min_cluster_size= 120\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12971\n",
      "For min_samples= 12  and min_cluster_size= 140\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12971\n",
      "For min_samples= 12  and min_cluster_size= 160\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 12971\n",
      "For min_samples= 12  and min_cluster_size= 180\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 26384\n",
      "For min_samples= 14  and min_cluster_size= 20\n",
      "Estimated number of clusters: 8\n",
      "Estimated number of noise points: 6813\n",
      "For min_samples= 14  and min_cluster_size= 40\n",
      "Estimated number of clusters: 3\n",
      "Estimated number of noise points: 4000\n",
      "For min_samples= 14  and min_cluster_size= 60\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 13138\n",
      "For min_samples= 14  and min_cluster_size= 80\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 13138\n",
      "For min_samples= 14  and min_cluster_size= 100\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 13138\n",
      "For min_samples= 14  and min_cluster_size= 120\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 13138\n",
      "For min_samples= 14  and min_cluster_size= 140\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 26552\n",
      "For min_samples= 14  and min_cluster_size= 160\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 26552\n",
      "For min_samples= 14  and min_cluster_size= 180\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 26552\n",
      "For min_samples= 16  and min_cluster_size= 20\n",
      "Estimated number of clusters: 6\n",
      "Estimated number of noise points: 4737\n",
      "For min_samples= 16  and min_cluster_size= 40\n",
      "Estimated number of clusters: 5\n",
      "Estimated number of noise points: 12982\n",
      "For min_samples= 16  and min_cluster_size= 60\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 13123\n",
      "For min_samples= 16  and min_cluster_size= 80\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 13123\n",
      "For min_samples= 16  and min_cluster_size= 100\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 13123\n",
      "For min_samples= 16  and min_cluster_size= 120\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 13123\n",
      "For min_samples= 16  and min_cluster_size= 140\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 26808\n",
      "For min_samples= 16  and min_cluster_size= 160\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 26808\n",
      "For min_samples= 16  and min_cluster_size= 180\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 26808\n",
      "For min_samples= 18  and min_cluster_size= 20\n",
      "Estimated number of clusters: 4\n",
      "Estimated number of noise points: 3806\n",
      "For min_samples= 18  and min_cluster_size= 40\n",
      "Estimated number of clusters: 4\n",
      "Estimated number of noise points: 13248\n",
      "For min_samples= 18  and min_cluster_size= 60\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 13352\n",
      "For min_samples= 18  and min_cluster_size= 80\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 13352\n",
      "For min_samples= 18  and min_cluster_size= 100\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 13352\n",
      "For min_samples= 18  and min_cluster_size= 120\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 26801\n",
      "For min_samples= 18  and min_cluster_size= 140\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 26801\n",
      "For min_samples= 18  and min_cluster_size= 160\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 26801\n",
      "For min_samples= 18  and min_cluster_size= 180\n",
      "Estimated number of clusters: 2\n",
      "Estimated number of noise points: 26801\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(min_samples_arr)):\n",
    "    for j in range(len(min_cluster_size_arr)):\n",
    "        # Apply HDBSCAN\n",
    "        min_samples = int(min_samples_arr[i])\n",
    "        min_cluster_size = int(min_cluster_size_arr[j])\n",
    "        extratext_f = extratext + '_min_samples_'+str(min_samples)\n",
    "        extratext_f += '_min_cluster_size_'+str(min_cluster_size)\n",
    "        clust = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size,  \n",
    "                        min_samples=min_samples,\n",
    "                        gen_min_span_tree=True, \n",
    "                        cluster_selection_method=cluster_selection_method).fit(X)\n",
    "        labels = clust.labels_ #[clust.ordering_]\n",
    "\n",
    "\n",
    "        # Number of clusters in labels, ignoring noise if present.\n",
    "        n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise_ = list(labels).count(-1)\n",
    "\n",
    "        print('For min_samples=',min_samples,' and min_cluster_size=',min_cluster_size)\n",
    "        print('Estimated number of clusters: %d' % n_clusters_)\n",
    "        print('Estimated number of noise points: %d' % n_noise_)\n",
    "\n",
    "\n",
    "        # ###########################################################################\n",
    "\n",
    "        # Black removed and is used for noise instead.\n",
    "        # unique_labels = set(labels)\n",
    "        # print(\"unique_labels are\", unique_labels)\n",
    "\n",
    "\n",
    "        # for k in unique_labels:\n",
    "        #     print(\"k is\", k)\n",
    "\n",
    "        #     class_member_mask = (labels == k)\n",
    "\n",
    "        #     if algorithm == 'DBSCAN':\n",
    "        #         mask = class_member_mask & core_samples_mask\n",
    "        #         anti_mask = class_member_mask & ~core_samples_mask\n",
    "        #     elif algorithm == 'OPTICS' or algorithm == 'HDBSCAN' or algorithm == 'AGG_n' or algorithm == 'AGG_l' or algorithm == 'GMM':\n",
    "        #         mask = class_member_mask\n",
    "        #         anti_mask = class_member_mask\n",
    "\n",
    "        #     xy = X[mask]\n",
    "        #     cluster_means[k] = np.mean(xy, axis = 0)\n",
    "        #     cluster_dispersions[k] = np.std(xy, axis = 0)\n",
    "        #     cluster_nstars[k] = len(X[class_member_mask])\n",
    "\n",
    "\n",
    "        # Generate the hierarchical tree if HDBSCAN\n",
    "        f = plt.figure(figsize=[20,10])\n",
    "        clust.condensed_tree_.plot(select_clusters=True)\n",
    "        f.savefig(plots_dir + 'hdbscan_clustering' + extratext_f + '_tree' + '.pdf')\n",
    "        plt.close(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
